{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e2cdae",
   "metadata": {},
   "source": [
    "# 6. 쳇봇엔진만들기\n",
    "\n",
    "## 6.1 쳇봇엔진구조\n",
    "\n",
    "<h6 align=\"center\">쳇봇 엔진의 핵심 기능</h6>\n",
    "\n",
    "|핵심기능|설명|\n",
    "|:------:|:----------------|\n",
    "|질문의도분류|화자의 질문의도를 파악, 해당 질문을 의도분류모델을 이용해 의도클래스를 예측하는 문제|\n",
    "|개체명 인식|화자의 질문에서 단어 토큰별 개체명을 인식. 이는 단어 토큰에 맞는 개체명을 예측하는 문제|\n",
    "|핵심 키워드 추출|화자질문에서 핵심단어토큰을 추출. 형태소분석기로 핵심 키워드가 되는 명사,동사를 추출|\n",
    "|답변 검색|해당질문의도, 개체명, 핵심키워드등을 기반으로 답변을 학습DB에서 검색|\n",
    "|소켓 서버|다향한종류(카카오톡, 네이버톡톡)의 챗봇 클라이언트에서 요청 질문을 처리하기 위해 소켓서버|\n",
    "||프로그램 역할을 한다. 따라서 이 책에서는 챗봇엔진 서버 프로그램이라 할 예정|\n",
    "\n",
    "## 6.2 쳇봇엔진처리과정\n",
    "\n",
    "1. 화자질의문장을 입력후 쳇봇엔진은 제일 먼저 전처리를 실행\n",
    "1. 형태소분석기를 통해 토큰을 추출후 필요한 품사(명사, 동사 등)이외의 불용어를 제거\n",
    "1. 의도분석과 개체명인식을 완료후 결과를 이용해서 적절한 답변을 학습된 DB에서 검색해서 화자에 답변을 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c231dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\utils\\Preprocess.py\n",
    "# Preprocess.py(1) - 전처리로직만 작성\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, userdic=None):\n",
    "        \n",
    "        # 형태소분석기객체생성 및 초기화\n",
    "        self.korman = Komoran(userdic=userdic)\n",
    "        \n",
    "        # 제외할 품사\n",
    "        # exclusion_tags리스트에 정의된 품사들은 불용어로 정의\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언 제거, 기호 제거\n",
    "        # 어미 제거\n",
    "        # 접미사 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            'JX', 'JC', 'SF', 'SP', 'SS', 'SE', 'SO', 'EP', \n",
    "            'EF', 'EC', 'ETN', 'ETM', 'XSN', 'XSV', 'XSA']  \n",
    "        \n",
    "    # 형태소분석기 POS 태거\n",
    "    def pos(self, sentence):\n",
    "        return self.korman.pos(sentence)\n",
    "        \n",
    "    # 불용어 제거후, 필요한 품사정보만 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if f(p[1]) is False:\n",
    "                word_list.append(p if without_tag is False else p[0])\n",
    "        return word_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇전처리테스트\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "sent = '내일 오전 10시에 탕수육 주문하고 싶어'\n",
    "\n",
    "# 1. 전처리객체생성\n",
    "p = Preprocess(userdic='./chatbot/utils/user_dic.tsv')\n",
    "\n",
    "# 2. 형태소분석\n",
    "pos = p.pos(sent)\n",
    "print(pos)\n",
    "\n",
    "# 3. 품사태그와 키워드를 출력\n",
    "ret = p.get_keywords(pos, without_tag=False)\n",
    "print(ret)\n",
    "\n",
    "# 4. 품사태그없이 키워드출력\n",
    "ret = p.get_keywords(pos, without_tag=True)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983fda0",
   "metadata": {},
   "source": [
    "## 6.3 단어사전구축과 시퀀스생성\n",
    "\n",
    "* 말뭉치데이터(corpus.txt) -> /train_tools/dict폴더에 저장\n",
    "* '내일' -> 999, '오전' -> 111의 형태로 시퀀스생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bdb689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\train_tools\\dict\\create_dict.py\n",
    "# create_dict.py(1) - 단어사전생성로직 작성\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "from tensorflow.keras import preprocessing\n",
    "import pickle\n",
    "\n",
    "# 1. 말뭉치데이터로딩 - 말뭉치파일을 list로 변환\n",
    "def reac_corpus_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:] # 헤더를 제거\n",
    "    return data\n",
    "\n",
    "# 2. 말뭉치데이터로딩\n",
    "corpus_data = reac_corpus_data('./chatbot/train_tools/dict/corpus.txt')\n",
    "\n",
    "# 3. 말뭉치데이터에서 키워드만 추출해서 사용자사전리스트를 생성\n",
    "# corpus_data리스트에서 POS태깅후 단어리스트(dict)에 저장\n",
    "p = Preprocess()\n",
    "d = []\n",
    "for c in corpus_data:\n",
    "    pos = p.pos(c[1])\n",
    "    for k in pos:\n",
    "        d.append(k[0])\n",
    "\n",
    "# 4. 사전에 사용될 index(word2index)를 생성 - 토크나이징을 해서 단어리스트(d)를 \n",
    "# 단어인덱스 딕셔너리(word_index)데이터를 생성\n",
    "# 사전의 첫 번째 인덱스에는 OOV를 설정\n",
    "tokenizer = preprocessing.text.Tokenizer(oov_token='OOV')\n",
    "tokenizer.fit_on_texts(d)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 5. 사전파일생성\n",
    "# 생성된 단어인덱스딕셔너리(word_index)객체를 파일로 저장\n",
    "f = open('./chatbot/train_tools/dict/chatbot_dict.bin', 'wb')\n",
    "try:\n",
    "    pickle.dump(word_index, f)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\train_tools\\dict\\create_dict_test.py\n",
    "# 단어사전테스트\n",
    "import pickle\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# 1. 단어사전로딩\n",
    "f = open('./chatbot/train_tools/dict/chatbot_dict.bin', 'rb')\n",
    "word_index = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "sent = \"내일 오전 10시에 탕수육 주문하고 싶어 ㅋㅋ\"\n",
    "\n",
    "# 2. 전처리객체생성\n",
    "p = Preprocess(userdic='../utils/user_dic.tsv')\n",
    "pos = p.pos(sent)\n",
    "\n",
    "# 3. 테스트문장을 입력값으로 키워드, index를 출력\n",
    "keywords = p.get_keywords(pos, without_tag=True)\n",
    "for word in keywords:\n",
    "    try:\n",
    "        print(word, word_index[word])\n",
    "    except KeyError:\n",
    "        # 해당단어가 사전에 없는 경우 OOV로 처리\n",
    "        print(word, word_index['OOV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\utils\\Preprocess.py\n",
    "# Preprocess.py(2) - 시퀀스생성로직 추가\n",
    "# 단어인덱스 시퀀스변환 메서드를 추가\n",
    "from konlpy.tag import Komoran\n",
    "import pickle\n",
    "import jpype\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, word2index_dic='', userdic=None):\n",
    "        # 단어 인덱스 사전 불러오기\n",
    "        if(word2index_dic != ''):\n",
    "            f = open(word2index_dic, \"rb\")\n",
    "            self.word_index = pickle.load(f)\n",
    "            f.close()\n",
    "        else:\n",
    "            self.word_index = None\n",
    "\n",
    "        # 형태소 분석기 초기화\n",
    "        self.komoran = Komoran(userdic=userdic)\n",
    "\n",
    "        # 제외할 품사\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언 제거, 기호 제거\n",
    "        # 어미 제거\n",
    "        # 접미사 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            'JX', 'JC', 'SF', 'SP', 'SS', 'SE', 'SO', 'EP', \n",
    "            'EF', 'EC', 'ETN', 'ETM', 'XSN', 'XSV', 'XSA']\n",
    "\n",
    "    # 형태소 분석기 POS 태거\n",
    "    def pos(self, sentence):\n",
    "        jpype.attachThreadToJVM()\n",
    "        return self.komoran.pos(sentence)\n",
    "\n",
    "    # 불용어 제거 후, 필요한 품사 정보만 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if f(p[1]) is False:\n",
    "                word_list.append(p if without_tag is False else p[0])\n",
    "        return word_list\n",
    "\n",
    "    # 키워드를 단어 인덱스 시퀀스로 변환\n",
    "    def get_wordidx_sequence(self, keywords):\n",
    "        if self.word_index is None:\n",
    "            return []\n",
    "\n",
    "        w2i = []\n",
    "        for word in keywords:\n",
    "            try:\n",
    "                w2i.append(self.word_index[word])\n",
    "            except KeyError:\n",
    "                # 해당 단어가 사전에 없는 경우, OOV 처리\n",
    "                w2i.append(self.word_index['OOV'])\n",
    "                \n",
    "        return w2i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf576b1",
   "metadata": {},
   "source": [
    "## 6.4 의도분류모델\n",
    "\n",
    "* 쳇봇엔진에 화자의 질의가 입력되었을 때 전처리과정을 거친후 `화자의 문장의 의도를 분류`해야 한다.\n",
    "* 클래스별로 분류하기위해서 `CNN모델을 사용`\n",
    "* 실습하는 말뭉치의 의도는 5가지분류\n",
    "\n",
    "<h6 align=\"center\">쳇봇 엔진의 의도 분류 클래스 종류</h6>\n",
    "\n",
    "|의도명|분류클래스|설명|\n",
    "|:------:|:---:|:----------------|\n",
    "|인사|0|텍스트가 인사말인 경우|\n",
    "|욕설|1|텍스트가 욕설인 경우|\n",
    "|주문|2|텍스트가 주문 관련 내용인 경우|\n",
    "|예약|3|텍스트가 예약 관련 내용인 경우|\n",
    "|기타|4|어떤 의도에도 포함되지 않는 경우|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\config\\GlobalParams.py\n",
    "# 글로벌 파라미터정보\n",
    "# 단어 시퀀스 벡터 크기\n",
    "MAX_SEQ_LEN = 15\n",
    "\n",
    "def GlobalParams():\n",
    "    global MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05eece",
   "metadata": {},
   "source": [
    "### 6.4.1 의도분류모델학습\n",
    "\n",
    "* 학습데이터 : ./chatbot/models/intent/total_train_data.csv\n",
    "  - 음식주문과 예약을 위한 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b486ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\models\\intent\\train_model.py\n",
    "# 쳇봇엔진-의도분류모델\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "# 1. 데이터로딩\n",
    "train_file = './chatbot/models/intent/total_train_data.csv'\n",
    "data = pd.read_csv(train_file, delimiter=\",\")\n",
    "queries = data['query'].tolist()\n",
    "intents = data['intent'].tolist()\n",
    "\n",
    "# 2. 단어시퀀스사전로딩\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "p = Preprocess(word2index_dic=\"./chatbot/train_tools/dict/chatbot_dict.bin\",\n",
    "               userdic=\"./chatbot/utils/user_dic.tsv\")\n",
    "\n",
    "# 3. 단어시퀀스 생성\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    pos = p.pos(sentence)\n",
    "    keywords = p.get_keywords(pos, without_tag=True)\n",
    "    seq = p.get_wordidx_sequence(keywords)\n",
    "    sequences.append(seq)\n",
    "    \n",
    "# 4. 단어인덱스시퀀스벡터생성\n",
    "# Preprocess에서 생성한 시퀀스벡터의 크기를 동일하게 처리하기 위해 \n",
    "# MAX_SEQ_LEN=15 즉 최대 벡터크기를 15로 설정하고 15이하인 벡터는 padding처리\n",
    "from chatbot.config.GlobalParams import MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "print(padded_seqs.shape, len(intents))\n",
    "print()\n",
    "\n",
    "# 5. 학습용, 검증용, 테스트용 데이터셋 생성 - 7:2:1\n",
    "# 패딩처리된 시퀀스(padded_seqs)리스트와 의도(intents)리스트를 데이터셋으로 생성후\n",
    "# 랜덤하게 학습용,검증용,테스트용 데이터셋을 7:2:1로 분할\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, intents))\n",
    "df = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "# 6. 하이퍼파라미터설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.word_index) + 1 \n",
    "\n",
    "# 7. CNN모델정의\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN, ))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "conv2 = Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "conv3 = Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "# 1) 3,4,5-gram 출력층을 한개로 합치기\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "# 2) 은닉층\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(5, name='logits')(dropout_hidden)\n",
    "predictions = Dense(5, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "# 8. CNN모델생성\n",
    "# 정의된 모델을 케라스모델에 추가, Model()함수는 입력층과 출력층만 사용\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 9. 모델학습\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
    "\n",
    "# 10. 모델평가\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print(f'Accuracy : {accuracy*100:.2f}')\n",
    "print(f'Loss     : {loss:.2f}')\n",
    "\n",
    "# 11. 모델저장\n",
    "model.save('./chatbot/models/intent/intent_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f99f06",
   "metadata": {},
   "source": [
    "### 6.4.2 의도분류모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\models\\intent\\IntentModel.py\n",
    "# 쳇봇엔진 - 의도분류모델\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "# 의도분류모델모듈\n",
    "class IntentModel:\n",
    "    \n",
    "    def __init__(self, model_name, preprocess):\n",
    "        \n",
    "        # 의도클래스별 레이블\n",
    "        self.labels = {0: '인사', 1: '욕설', 2: '주문', 3: '예약', 4: '기타'}\n",
    "        \n",
    "        # 의도분류모델로딩\n",
    "        self.model = load_model(model_name)\n",
    "        \n",
    "        # 챗봇 Preprocess\n",
    "        self.p = preprocess\n",
    "    \n",
    "    # 의도클래스 예측 함수\n",
    "    def predict_class(self, query):\n",
    "        \n",
    "        # 형태소분석\n",
    "        pos = self.p.pos(query)\n",
    "        \n",
    "        # 문장내 키워드 출출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "    \n",
    "        # 벡터최대크기\n",
    "        from chatbot.config.GlobalParams import MAX_SEQ_LEN\n",
    "        \n",
    "        # 패딩처리\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "        \n",
    "        predict = self.model.predict(padded_seqs)\n",
    "        predict_class = tf.math.argmax(predict, axis=1)\n",
    "        \n",
    "        return predict_class.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd24432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\models\\intent\\model_intent_test.py\n",
    "# 쳇봇엔진 - 의도분류모델 테스트\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "from chatbot.models.intent.IntentModel import IntentModel\n",
    "\n",
    "p = Preprocess(word2index_dic=\"./chatbot/train_tools/dict/chatbot_dict.bin\",\n",
    "               userdic=\"./chatbot/utils/user_dic.tsv\")\n",
    "\n",
    "intent = IntentModel(model_name='./chatbot/models/intent/intent_model.keras', preprocess=p)\n",
    "\n",
    "query = '오늘 탕수육 주문 가능한가요?'\n",
    "predict = intent.predict_class(query)\n",
    "predict_label = intent.labels[predict]\n",
    "\n",
    "print(query)\n",
    "print(f'질의를 예측한 의도클래스 = {predict}')\n",
    "print(f'질의를 예측한 의도레이블 = {predict_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1ca55",
   "metadata": {},
   "source": [
    "## 6.5 개체명인식모델학습\n",
    "\n",
    "* 개체명인식모델을 `양방향LSTM모델을 사용`\n",
    "\n",
    "<h6 align=\"center\">개체명종류</h6>\n",
    "\n",
    "|개체명|설명|\n",
    "|:----:|:-----------------|\n",
    "|B_FOOD|음식|\n",
    "|B_DT, B_TI|날짜,시간(학습데이터의 영향으로 날짜와 시간을 혼용해서 사용)|\n",
    "|B_PS|사람|\n",
    "|B_OG|조직, 회사|\n",
    "|B_LC|지역|\n",
    "\n",
    "### 6.5.1 개체명인식모델 데이터셋\n",
    "\n",
    "* 학습용데이터셋 : ./chatbot/models/ner/ner_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\models\\ner\\train_model.py\n",
    "# 쳇봇엔진 - NER모델작성\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# 1. 학습파일로딩\n",
    "def read_file(file_name):\n",
    "    sents = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, l in enumerate(lines):\n",
    "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
    "                this_sent = []\n",
    "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
    "                continue\n",
    "            elif l[0] == '\\n':\n",
    "                sents.append(this_sent)\n",
    "            else:\n",
    "                this_sent.append(tuple(l.split()))\n",
    "    return sents\n",
    "\n",
    "p = Preprocess(word2index_dic=\"./chatbot/train_tools/dict/chatbot_dict.bin\",\n",
    "               userdic=\"./chatbot/utils/user_dic.tsv\")\n",
    "\n",
    "# 2. 학습용말뭉치데이터 로딩\n",
    "corpus = read_file('./chatbot/models/ner/ner_train.txt')\n",
    "\n",
    "# 3. 말뭉치데이터에서 단어(2번쨰), BIO태그(4번째)만 로딩해서 학습용데이터셋을 생성\n",
    "sentences, tags = [], []\n",
    "for t in corpus:\n",
    "    tagged_sentence = []\n",
    "    sentence, bio_tag = [], []\n",
    "    for w in t:\n",
    "        tagged_sentence.append((w[1], w[3]))\n",
    "        sentence.append(w[1])\n",
    "        bio_tag.append(w[3])\n",
    "    \n",
    "    sentences.append(sentence)\n",
    "    tags.append(bio_tag)\n",
    "    \n",
    "print(\"샘플 크기 : \\n\", len(sentences))\n",
    "print(\"0번 째 샘플 단어 시퀀스 : \\n\", sentences[0])\n",
    "print(\"0번 째 샘플 bio 태그 : \\n\", tags[0])\n",
    "print(\"샘플 단어 시퀀스 최대 길이 :\", max(len(l) for l in sentences))\n",
    "print(\"샘플 단어 시퀀스 평균 길이 :\", (sum(map(len, sentences))/len(sentences)))    \n",
    "\n",
    "# 4. 토크나이저 정의 \n",
    "# 단어시퀀스는 Preprocess객체에서 생성하기 때문에 예제에서는 BIO태그용 토크나이저 객체만 생성\n",
    "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower=False 소문자로 변환하지 않는다.\n",
    "tag_tokenizer.fit_on_texts(tags)\n",
    "\n",
    "# 단어사전 및 태그 사전 크기\n",
    "vocab_size = len(p.word_index) + 1\n",
    "tag_size = len(tag_tokenizer.word_index) + 1\n",
    "print(\"BIO 태그 사전 크기 :\", tag_size)\n",
    "print(\"단어 사전 크기 :\", vocab_size)\n",
    "\n",
    "# 5. 학습용단어시퀀스생성\n",
    "# BIO태그는 토크나이저에서 생성된 사전데이터를 시퀀스번호형태로 인코딩한다.\n",
    "x_train = [p.get_wordidx_sequence(sent) for sent in sentences]\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "\n",
    "index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환 하기 위해 사용\n",
    "index_to_ner[0] = 'PAD'\n",
    "\n",
    "# 6. 시퀀스 패딩 처리\n",
    "# 개체명인식모델의 입출력크기를 동일하게 맞추기위해 시퀀스 패딩작업을 진행\n",
    "# 벡터크기를 단어 시퀀스의 평균길이보다 여유있게 40으로 정의\n",
    "max_len = 40\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=max_len)\n",
    "y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)\n",
    "\n",
    "# 7. 학습 데이터와 테스트 데이터를 8:2의 비율로 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,\n",
    "                                                    test_size=.2,\n",
    "                                                    random_state=1234)\n",
    "\n",
    "# 8. 출력 데이터를 one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
    "\n",
    "print(\"학습 샘플 시퀀스 형상 : \", x_train.shape)\n",
    "print(\"학습 샘플 레이블 형상 : \", y_train.shape)\n",
    "print(\"테스트 샘플 시퀀스 형상 : \", x_test.shape)\n",
    "print(\"테스트 샘플 레이블 형상 : \", y_test.shape)\n",
    "\n",
    "# 9. 모델정의(Bi-LSTM)\n",
    "# tag_size만큼 출력 뉴런에서 제일 확률 높은 출력값 1개를 선택하기 위해 활성화함수로 softmax를 사용\n",
    "# 손실함수는 categorical_crossentropy를 사용\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "\n",
    "print(\"평가 결과 : \", model.evaluate(x_test, y_test)[1])\n",
    "model.save('./chatbot/models/ner/ner_model.keras')\n",
    "\n",
    "# 10. 시퀀스를 NER태그로 변환\n",
    "# 예측값을 index_to_ner함수를 사용해서 태깅정보로 변경하는 함수\n",
    "def sequences_to_tag(sequences):\n",
    "    result = []\n",
    "    for sequence in sequences: # 전체 시퀀스에서 시퀀스를 하나씩 꺼내기\n",
    "        temp = []\n",
    "        for pred in sequence:  # 시퀀스로 부터 예측값을 하나씩 꺼낵;\n",
    "            pred_index = np.argmax(pred) # [0,0,1,...]라면 1의 위치인 2를 리턴\n",
    "            temp.append(index_to_ner[pred_index].replace('PAD', 'O')) # 품사태그 PAD를 O로 변경\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "# 11. 테스트데이터셋의 NER예측\n",
    "# 1) f1_sre를 계산하기 위한 모듈 import\n",
    "#    f1_score계산을 위해 predict()함수를 사용\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "# 2) 테스트데이터셋으로 예측\n",
    "#    x_test(테스트용데이터셋)는 시퀀스번호로 인코딩된 데이터(단어시퀀스, numpy배열)\n",
    "#    테스트한 결과로 예측된 NER태그정보가 담긴 넘파이배열을 리턴\n",
    "y_predicted = model.predict(x_test)\n",
    "pred_tags = sequences_to_tag(y_predicted) # 예측된 개체인식명(NER)\n",
    "test_tags = sequences_to_tag(y_test)      # 실제 개체인식명\n",
    "\n",
    "# 3) f1 평가결과\n",
    "#    classification_report함수로 NER태그별로 계산된 정밀도, 재현율, f1-score를 출력\n",
    "print(classification_report(test_tags, pred_tags))\n",
    "print(f'f1 score : {f1_score(test_tags, pred_tags):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ab5fb",
   "metadata": {},
   "source": [
    "### 6.5.2 개체명 인식 모듈 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./chatbot/models/ner/NerModel.py\n",
    "# 쳇봇엔진의 NER모델\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "# 개체명 인식 모델 모듈\n",
    "class NerModel:\n",
    "    \n",
    "    # 생성자\n",
    "    def __init__(self, model_name, preprocess):\n",
    "        \n",
    "        # BIO태그클래스별 레이블정의\n",
    "        self.index_to_ner = {1: 'O', 2: 'B_DT', 3: 'B_FOOD', 4: 'I', 5: 'B_OG', \n",
    "                             6: 'B_PS', 7: 'B_LC', 8: 'NNP', 9: 'B_TI', 0: 'PAD',}\n",
    "        \n",
    "        # 의도분류모델을 로딩\n",
    "        self.model = load_model(model_name)\n",
    "\n",
    "        # 쳇봇 Preprocess객체\n",
    "        self.p = preprocess\n",
    "    \n",
    "    \n",
    "    # 개체명클래스예측\n",
    "    def predict(self, query):\n",
    "        \n",
    "        # 형태소분석\n",
    "        pos = self.p.pos(query)\n",
    "        \n",
    "        # 문장내 키워드 추출 & 불용어 제거\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "        \n",
    "        # 패딩처리\n",
    "        max_len = 40\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, \n",
    "                                                           padding='post', value=0, maxlen=max_len)\n",
    "        \n",
    "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
    "        predict_class = tf.math.argmax(predict, axis=-1)\n",
    "        \n",
    "        tags = [self.index_to_ner[i] for i in predict_class.numpy()[0]]\n",
    "        \n",
    "        return list(zip(keywords, tags))\n",
    "        \n",
    "              \n",
    "    # 예측된 클래스를 태깅\n",
    "    def predict_tags(self, query):\n",
    "        \n",
    "        # 형태소분석\n",
    "        pos = self.p.pos(query)\n",
    "        \n",
    "        # 문장내 키워드 추출 & 불용어 제거\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "        \n",
    "         # 패딩처리\n",
    "        max_len = 40\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, \n",
    "                                                           padding='post', value=0, maxlen=max_len)\n",
    "        \n",
    "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
    "        predict_class = tf.math.argmax(predict, axis=-1)   \n",
    "        \n",
    "        tags = []\n",
    "        for tag_idx in predict_class.numpy()[0]:\n",
    "            if tag_idx == 1: continue\n",
    "            tags.append(self.index_to_ner[tag_idx])\n",
    "            \n",
    "        if len(tags) == 0: return None\n",
    "        \n",
    "        return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779786ab",
   "metadata": {},
   "source": [
    "##### NerModel.py 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./chatbot/test/model_intent_test.py\n",
    "# NerModel모듈 사용(1)\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "from chatbot.models.intent.IntentModel import IntentModel\n",
    "\n",
    "p = Preprocess(word2index_dic=\"./chatbot/train_tools/dict/chatbot_dict.bin\",\n",
    "               userdic=\"./chatbot/utils/user_dic.tsv\")\n",
    "\n",
    "ner = NerModel(model_name='./chatbot/models/ner/ner_model.keras', preprocess=p)\n",
    "query = '오늘 오전 13시 10분에 탕수육을 주문하고 싶어요'\n",
    "predicts = ner.predict(query)\n",
    "print(predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90686c",
   "metadata": {},
   "source": [
    "## 6.6 답변검색\n",
    "\n",
    "* 발화자로 부터 입력된 문장을 전처리, 의도분류, 개체명인식과정을 통해 적절한 답변을 학습DB에서 검색\n",
    "* 쳇봇엔진이 자연어처리를 통해서 해석한 문장을 기초로 유사한 답변을 검색(DB에서 검색)\n",
    "* 실제 상용화된 쳇봇은 여러가지 여건상 구현하기가 힘들기 때문에 단순한 검색수준의 SQL을 이용한 DB기반으로 답변을 검색하는 방법을 구현\n",
    "\n",
    "### 6.6.1 DB제어모듈생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfdfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./chatbot/utils/Database.py\n",
    "# DB제어모듈\n",
    "import pymysql\n",
    "import pymysql.cursors\n",
    "import logging\n",
    "\n",
    "class Database:\n",
    "    '''\n",
    "        Database Controll Module...\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, host, user, password, db_name, charset='utf8'):\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.db_name = db_name\n",
    "        self.charset = charset\n",
    "        self.conn = None\n",
    "        \n",
    "    def connect(self):\n",
    "        if self.conn != None:\n",
    "            return\n",
    "        \n",
    "        self.conn = pymysql.connect(\n",
    "            host = self.host,\n",
    "            user = self.user,\n",
    "            password = self.password,\n",
    "            db = self.db_name,\n",
    "            charset = self.charset\n",
    "        )\n",
    "        \n",
    "    def close(self):\n",
    "        if self.conn is None:\n",
    "            return\n",
    "        \n",
    "        if not self.conn.open:\n",
    "            self.conn = None\n",
    "            return\n",
    "        \n",
    "        self.conn.close()\n",
    "        self.conn = None\n",
    "    \n",
    "    # insert, delete, update\n",
    "    def execute(self, sql):\n",
    "        last_row_id = -1\n",
    "        try:\n",
    "            with self.conn.cursor() as cursor:\n",
    "                cursor.execute(sql)\n",
    "            self.conn.commit()\n",
    "            last_row_id = cursor.lastrowid\n",
    "            # logging.debug(\"execute last_row_id : %d\", last_row_id)\n",
    "        except Exception as e:\n",
    "            logging.error(e)\n",
    "    \n",
    "    # select one\n",
    "    def select_one(self, sql):\n",
    "        \n",
    "        result = None\n",
    "        \n",
    "        try:\n",
    "            with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "                cursor.execute(sql)\n",
    "                result = cursor.fetchone()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(e)    \n",
    "            \n",
    "        finally:\n",
    "            return result\n",
    "        \n",
    "    \n",
    "    # select_all\n",
    "    def select_all(self, sql):\n",
    "        \n",
    "        result = None\n",
    "        \n",
    "        try:\n",
    "            with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "                cursor.execute(sql)\n",
    "                result = cursor.fetchall()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(e)    \n",
    "            \n",
    "        finally:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ade30",
   "metadata": {},
   "source": [
    "### 6.6.2 답변검색모듈생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c6394",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./chatbot/utils/FindAnswer.py\n",
    "# 답변검색모듈\n",
    "\n",
    "class FindAnswer:\n",
    "    \n",
    "    def __init__(self, db):\n",
    "        self.db = db\n",
    "        \n",
    "    # 검색 sql생성\n",
    "    def make_query(self, intent_name, ner_tags):\n",
    "        sql = \"select * from chatbot_train_data \"\n",
    "        if intent_name != None and ner_tags == None:\n",
    "            sql = sql + f\" where intent = '{intent_name}'\"\n",
    "            \n",
    "        elif intent_name != None and ner_tags != None:\n",
    "            where = f\" where intent = '{intent_name}'\"\n",
    "            if(len(ner_tags) > 0):\n",
    "                where += \" and (\"\n",
    "                for ne in ner_tags:\n",
    "                    where += f\" ner like '%{ne}%' or \"\n",
    "                where = where[:-3] + \")\"\n",
    "                \n",
    "            sql = sql + where\n",
    "        \n",
    "        # 동일한 답변이 2개이상인 경우, 랜덤으로 선택\n",
    "        sql = sql + \" order by rand() limit 1\"\n",
    "        \n",
    "        return sql\n",
    "        \n",
    "    # 답변검색\n",
    "    # 의도명(intent_name)과 태그리스트(ner_tags)를 이용해서 질문의 답변을 검색하는 메서드\n",
    "    def search(self, intent_name, ner_tags):\n",
    "        \n",
    "        # 의도명, 개체인식명으로 답변을 검색\n",
    "        sql = self.make_query(intent_name, ner_tags)\n",
    "        answer = self.db.select_one(sql)\n",
    "        \n",
    "        # 검색되는 답변이 없을 경우 의도명만 검색\n",
    "        if answer is None:\n",
    "            sql = self.make_query(intent_name, None)\n",
    "            answer = self.db.select_one(sql)\n",
    "            \n",
    "        return(answer['answer'], answer['answer_image'])\n",
    "   \n",
    "    # NER태그를 실제로 입력된 단어로 변환하는 함수\n",
    "    # 질물 : 탕수육 대자로 한개 주문할게요 -> 개체인식명 탕수육 B_FOOD으로 처리\n",
    "    # 답변 : {B_FOOD} 주문 처리 완료되었습니다. 주문해주셔서 감사합니다.\n",
    "    def tag_to_word(self, ner_predicts, answer):\n",
    "        for word, tag in ner_predicts:\n",
    "            \n",
    "            # 변환해야 하는 태그가 있는 경우 추가\n",
    "            if tag == 'B_FOOD' or tag == 'B_DT' or tag == 'B_TI':\n",
    "                answer = answer.replace(tag, word)\n",
    "                \n",
    "        answer = answer.replace('{', '')\n",
    "        answer = answer.replace('}', '')\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a51b04",
   "metadata": {},
   "source": [
    "### 6.6.3 쳇봇엔진 동작 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7d344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%writefile ./chatbot/test/chatbot_test.py\n",
    "# 쳇봇엔진동작 테스트하기\n",
    "from chatbot.config.DatabaseConfig import *\n",
    "from chatbot.utils.Database import Database\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# 전처리객체생성\n",
    "p = Preprocess(word2index_dic=\"./chatbot/train_tools/dict/chatbot_dict.bin\",\n",
    "               userdic=\"./chatbot/utils/user_dic.tsv\")\n",
    "\n",
    "# DB객체생성\n",
    "db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "db.connect()\n",
    "\n",
    "# 발화자질문\n",
    "# query = '오전에 탕수육 10개 주문합니다'\n",
    "query = '오전에 탕수육 주문합니다'\n",
    "# query = '화자의 질문의도를 파악합니다'\n",
    "# query = \"안녕하세요\"\n",
    "# query = '자장면 주문할게요!'\n",
    "\n",
    "# 발화자의 의도파악\n",
    "from chatbot.models.intent.IntentModel import IntentModel\n",
    "intent = IntentModel(model_name ='./chatbot/models/intent/intent_model.keras', preprocess=p)\n",
    "predict = intent.predict_class(query)\n",
    "intent_name = intent.labels[predict]\n",
    "\n",
    "# 개체명인식\n",
    "from chatbot.models.ner.NerModel import NerModel\n",
    "ner = NerModel(model_name='./chatbot/models/ner/ner_model.keras', preprocess=p)\n",
    "predicts = ner.predict(query)\n",
    "ner_tags = ner.predict_tags(query)\n",
    "\n",
    "# 출력확인\n",
    "print(f\"질문 : {query}\")\n",
    "print(f\"의도파악 : {intent_name}\")\n",
    "print(f\"개체명인식 : {predicts}\")\n",
    "print(f\"답변검색에 필요한 NER태그 = {ner_tags}\")\n",
    "print()\n",
    "\n",
    "# 답변검색\n",
    "from chatbot.utils.FindAnswer import FindAnswer\n",
    "\n",
    "try:\n",
    "    f = FindAnswer(db)\n",
    "    # print(f.make_query(intent_name, ner_tags))\n",
    "    answer_text, answer_image = f.search(intent_name, ner_tags)\n",
    "    answer = f.tag_to_word(predicts, answer_text)\n",
    "except:\n",
    "    answer = \"죄송합니다. 무슨 말인지 모르겠습니다!\"\n",
    "    \n",
    "print(f\"답변 = {answer}\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee752db",
   "metadata": {},
   "source": [
    "## 6.7 챗봇엔진서버\n",
    "\n",
    "* 챗봇API인 카카카오톡이나 네이버톡톡같은 메신저 플랫폼을 이용\n",
    "* 실습은 카카오톡 API를 사용\n",
    "\n",
    "### 6.7.1 통신프로토콜정의\n",
    "\n",
    "* 서버와 클라이언트간 JSON형태로 통신\n",
    "\n",
    "```json\n",
    "# 질의 텍스트\n",
    "{\n",
    "    \"Query\": \"자장면 주문할게요\",\n",
    "    \"BotType\": \"Kakao\"\n",
    "}\n",
    "\n",
    "# 답변\n",
    "{\n",
    "    \"Query\": \"자장면 주문할게요\",\n",
    "    \"Intent\": \"주문\",\n",
    "    \"NER\":\"[('자장면', 'B_FOOD'), ('주문', 'O)]\",\n",
    "    \"Answer\":\"자장면 주문 처리 감사\",\n",
    "    \"AnswerImageUrl\":\"\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833df09e",
   "metadata": {},
   "source": [
    "### 6.7.2 챗봇서버모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c08ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./chatbot/utils/BotServer.py\n",
    "# 챗봇서버모듈\n",
    "import socket\n",
    "\n",
    "class BotServer:\n",
    "    \n",
    "    # 1. 챗봇의 서버포트와 동시접속자수를 정의\n",
    "    def __init__(self,srv_port, listen_num):\n",
    "        self.port = srv_port     # 서버포트\n",
    "        self.listen = listen_num # 서버에 동시접속자수\n",
    "        self.mySock = None\n",
    "        \n",
    "    # 2. socket생성\n",
    "    #    파이썬에서 지원하는 저수준 네트워킹인터페이스 API를 사용하기 쉽도록 작성한 레퍼함수\n",
    "    #    TCP/IP 소켓생성후 접속자수만큼 클라이어트의 연결을 수락\n",
    "    def create_socket(self):\n",
    "        self.mySock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.mySock.bind((\"0.0.0.0\", int(self.port)))\n",
    "        self.mySock.listen(int(self.listen))\n",
    "        return self.mySock\n",
    "        \n",
    "    # 3. 클라이언트 대기후 연결을 수락하는 메서드\n",
    "    #    연결요청시 클라이언트와 통신가능한 소켓객체를 리턴\n",
    "    #    반환값(conn, address)을 튜플형태로 리턴\n",
    "    def ready_for_client(self):\n",
    "        return self.mySock.accept()\n",
    "    \n",
    "    # 4. sock반환\n",
    "    def get_sock(self):\n",
    "        return self.mySock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9d42d",
   "metadata": {},
   "source": [
    "### 6.7.3 챗봇서버 메인 프로그램\n",
    "\n",
    "* cmd창에서 : python bot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./bot.py\n",
    "# 쳇봇서버메인프로그램\n",
    "import threading\n",
    "import json\n",
    "\n",
    "from chatbot.config.DatabaseConfig import *\n",
    "from chatbot.utils.Database import Database\n",
    "from chatbot.utils.BotServer import BotServer\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "from chatbot.models.intent.IntentModel import IntentModel\n",
    "from chatbot.models.ner.NerModel import NerModel\n",
    "from chatbot.utils.FindAnswer import FindAnswer\n",
    "\n",
    "# 1. 전처리\n",
    "p = Preprocess(word2index_dic=\"./chatbot/train_tools/dict/chatbot_dict.bin\",\n",
    "               userdic=\"./chatbot/utils/user_dic.tsv\")\n",
    "\n",
    "# 2. 의도파악 학습모델\n",
    "intent = IntentModel(model_name ='./chatbot/models/intent/intent_model.keras', preprocess=p)\n",
    "\n",
    "# 3. 개체인식 학습모델\n",
    "ner = NerModel(model_name='./chatbot/models/ner/ner_model.keras', preprocess=p)\n",
    "\n",
    "# 4. 클라이언트가 연결되는 순간 실행되는 Thread함수\n",
    "#    적절한 답변을 검색후 클라이언트에 Response\n",
    "def to_client(conn, addr, params):\n",
    "    \n",
    "    db = params['db']\n",
    "    \n",
    "    try:\n",
    "        db.connect()\n",
    "        \n",
    "        # 1) 데이터수신\n",
    "        read = conn.recv(2048)   # 수신데이터가 있을 때까지 블로킹\n",
    "        # conn은 쳇봇클라이언트소켁객체, recv()메서드는 데이터가 수신될 때까지 블러킹\n",
    "        # 최대 2048바이트만큼 데이터를 수신\n",
    "        # 연결중단 or 오류가 있을 경우 블러킹이 해제 되어 None 리턴\n",
    "        print(\"=\"*60)\n",
    "        print(f'Connection from : {str(addr)}')\n",
    "        \n",
    "        if read is None or not read:  # 클라이언트연결이 끊어지거나 오류가 있을 경우\n",
    "            print('클라이언트 연결 끊어짐')\n",
    "            exit(0)\n",
    "            \n",
    "        # 2) json데이터로 변환\n",
    "        recv_json_data = json.loads(read.decode())\n",
    "        print(f'데이터수신 : {recv_json_data}')\n",
    "        query = recv_json_data[\"Query\"]\n",
    "        \n",
    "        # 3) 의도파악\n",
    "        intent_predict = intent.predict_class(query)\n",
    "        intent_name = intent.labels[intent_predict]\n",
    "        \n",
    "        # 4) 개체명 파악\n",
    "        ner_predicts = ner.predict(query)\n",
    "        ner_tags = ner.predict_tags(query)\n",
    "        \n",
    "        # 5) 답변검색\n",
    "        try:\n",
    "            f = FindAnswer(db)\n",
    "            answer_text, answer_image = f.search(intent_name, ner_tags)\n",
    "            answer = f.tag_to_word(ner_predicts, answer_text)\n",
    "        except:\n",
    "            answer = \"죄송합니다. 무슨 말인지 모르겠어오 조금 더 학습을 할게요!!\"\n",
    "            answer_image = None\n",
    "            \n",
    "        send_json_data_str = {\n",
    "            \"Query\": query,\n",
    "            \"Answer\": answer,\n",
    "            \"AnswerImageUrl\": answer_image,                   \n",
    "            \"Intent\": intent_name,\n",
    "            \"NER\": ner_tags    \n",
    "        }\n",
    "        \n",
    "        message = json.dumps(send_json_data_str)\n",
    "        conn.send(message.encode())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    finally:\n",
    "        \n",
    "        if db is not None:\n",
    "            db.close()\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 1) 답변검색을 위한 DB연결\n",
    "    db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "    print(\"DB접속성공!!!\")\n",
    "    \n",
    "    port = 5050\n",
    "    listen = 100\n",
    "    \n",
    "    # 2) 챗봇서버동작 - 챗봇클라이언트 연결을 대기(무한 Loop)\n",
    "    bot = BotServer(port, listen)\n",
    "    bot.create_socket()\n",
    "    print(\"챗봇서버시작!!\")\n",
    "    \n",
    "    while True:\n",
    "        conn, addr = bot.ready_for_client()\n",
    "        params = {\n",
    "            \"db\": db\n",
    "        }\n",
    "        \n",
    "        client = threading.Thread(target=to_client, args=(conn, addr, params))\n",
    "        client.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c9abe",
   "metadata": {},
   "source": [
    "### 6.7.4 챗봇 클라이언트 프로그램\n",
    "\n",
    "* cmd창에서 : python ./chatbot/test/chatbot_client_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2880c963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./chatbot/test/chatbot_client_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./chatbot/test/chatbot_client_test.py\n",
    "# 챗봇 클라이언트 테스트 프로그램\n",
    "import socket\n",
    "import json\n",
    "\n",
    "# 쳇봇엔진서버접속정보\n",
    "host = \"127.0.0.1\"\n",
    "port = 5050\n",
    "\n",
    "# 클라이언트프로그램시작\n",
    "while True:\n",
    "    \n",
    "    query = input(\"질문을 입력하세요 => \") # 질문입력\n",
    "    print(f\"질문 : {query}\")\n",
    "    if(query=='q'): exit(0)\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    mySocket = socket.socket()\n",
    "    mySocket.connect((host, port))\n",
    "    \n",
    "    # 쳇봇엔진에 질의요청\n",
    "    json_data = {\n",
    "        'Query': query,\n",
    "        'BotType': \"myBotService\"\n",
    "    }\n",
    "    message = json.dumps(json_data)\n",
    "    mySocket.send(message.encode())\n",
    "        \n",
    "    # 쳇봇엔진답변출력\n",
    "    data = mySocket.recv(2048).decode()\n",
    "    ret_data = json.loads(data)\n",
    "    print(f\"답변 : {ret_data['Answer']}\")\n",
    "    print(type(ret_data), ret_data)\n",
    "    \n",
    "    # 쳇봇엔진서버연결된 소켓 닫기\n",
    "    mySocket.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d144f2",
   "metadata": {},
   "source": [
    "## 6.8 맺음말\n",
    "\n",
    "* 실제 음식주문용챗봇을 사용하려면 발화자의 요청이 2개 이상의 B_FOOD를 인식해야 하고\n",
    "* 음식주문 수량을 확인할 수 있는 개체명이 추가되어야 한다.\n",
    "* 현재, 실습한 음식챗봇은 `자장면 1개, 탕수육 대 1개 주문할게요`의 주문을 정확하게 처리할 수 없다.\n",
    "* 정확한 주문을 처리하는 챗봇을 만들기 위해서 학습데이터와 개체명인식데이터가 필요하다\n",
    "* 이와같이 딥러닝모델에서는 학습용데이터가 매우 중요하다.\n",
    "* 우리가 목표로 하는 시스템에 맞는 데이터수집, 정제하는데 대부분의 시간이 필요하다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
