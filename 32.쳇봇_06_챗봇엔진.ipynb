{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e2cdae",
   "metadata": {},
   "source": [
    "# 6. 쳇봇엔진만들기\n",
    "\n",
    "## 6.1 쳇봇엔진구조\n",
    "\n",
    "<h6 align=\"center\">쳇봇 엔진의 핵심 기능</h6>\n",
    "\n",
    "|핵심기능|설명|\n",
    "|:------:|:----------------|\n",
    "|질문의도분류|화자의 질문의도를 파악, 해당 질문을 의도분류모델을 이용해 의도클래스를 예측하는 문제|\n",
    "|개체명 인식|화자의 질문에서 단어 토큰별 개체명을 인식. 이는 단어 토큰에 맞는 개체명을 예측하는 문제|\n",
    "|핵심 키워드 추출|화자질문에서 핵심단어토큰을 추출. 형태소분석기로 핵심 키워드가 되는 명사,동사를 추출|\n",
    "|답변 검색|해당질문의도, 개체명, 핵심키워드등을 기반으로 답변을 학습DB에서 검색|\n",
    "|소켓 서버|다향한종류(카카오톡, 네이버톡톡)의 챗봇 클라이언트에서 요청 질문을 처리하기 위해 소켓서버|\n",
    "||프로그램 역할을 한다. 따라서 이 책에서는 챗봇엔진 서버 프로그램이라 할 예정|\n",
    "\n",
    "## 6.2 쳇봇엔진처리과정\n",
    "\n",
    "1. 화자질의문장을 입력후 쳇봇엔진은 제일 먼저 전처리를 실행\n",
    "1. 형태소분석기를 통해 토큰을 추출후 필요한 품사(명사, 동사 등)이외의 불용어를 제거\n",
    "1. 의도분석과 개체명인식을 완료후 결과를 이용해서 적절한 답변을 학습된 DB에서 검색해서 화자에 답변을 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c231dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\utils\\Preprocess.py\n",
    "# Preprocess.py(1) - 전처리로직만 작성\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, userdic=None):\n",
    "        \n",
    "        # 형태소분석기객체생성 및 초기화\n",
    "        self.korman = Komoran(userdic=userdic)\n",
    "        \n",
    "        # 제외할 품사\n",
    "        # exclusion_tags리스트에 정의된 품사들은 불용어로 정의\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언 제거, 기호 제거\n",
    "        # 어미 제거\n",
    "        # 접미사 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            'JX', 'JC', 'SF', 'SP', 'SS', 'SE', 'SO', 'EP', \n",
    "            'EF', 'EC', 'ETN', 'ETM', 'XSN', 'XSV', 'XSA']  \n",
    "        \n",
    "    # 형태소분석기 POS 태거\n",
    "    def pos(self, sentence):\n",
    "        return self.korman.pos(sentence)\n",
    "        \n",
    "    # 불용어 제거후, 필요한 품사정보만 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if f(p[1]) is False:\n",
    "                word_list.append(p if without_tag is False else p[0])\n",
    "        return word_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇전처리테스트\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "sent = '내일 오전 10시에 탕수육 주문하고 싶어'\n",
    "\n",
    "# 1. 전처리객체생성\n",
    "p = Preprocess(userdic='./chatbot/utils/user_dic.tsv')\n",
    "\n",
    "# 2. 형태소분석\n",
    "pos = p.pos(sent)\n",
    "print(pos)\n",
    "\n",
    "# 3. 품사태그와 키워드를 출력\n",
    "ret = p.get_keywords(pos, without_tag=False)\n",
    "print(ret)\n",
    "\n",
    "# 4. 품사태그없이 키워드출력\n",
    "ret = p.get_keywords(pos, without_tag=True)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983fda0",
   "metadata": {},
   "source": [
    "## 6.3 단어사전구축과 시퀀스생성\n",
    "\n",
    "* 말뭉치데이터(corpus.txt) -> /train_tools/dict폴더에 저장\n",
    "* '내일' -> 999, '오전' -> 111의 형태로 시퀀스생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bdb689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\train_tools\\dict\\create_dict.py\n",
    "# create_dict.py(1) - 단어사전생성로직 작성\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "from tensorflow.keras import preprocessing\n",
    "import pickle\n",
    "\n",
    "# 1. 말뭉치데이터로딩 - 말뭉치파일을 list로 변환\n",
    "def reac_corpus_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:] # 헤더를 제거\n",
    "    return data\n",
    "\n",
    "# 2. 말뭉치데이터로딩\n",
    "corpus_data = reac_corpus_data('./chatbot/train_tools/dict/corpus.txt')\n",
    "\n",
    "# 3. 말뭉치데이터에서 키워드만 추출해서 사용자사전리스트를 생성\n",
    "# corpus_data리스트에서 POS태깅후 단어리스트(dict)에 저장\n",
    "p = Preprocess()\n",
    "d = []\n",
    "for c in corpus_data:\n",
    "    pos = p.pos(c[1])\n",
    "    for k in pos:\n",
    "        d.append(k[0])\n",
    "\n",
    "# 4. 사전에 사용될 index(word2index)를 생성 - 토크나이징을 해서 단어리스트(d)를 \n",
    "# 단어인덱스 딕셔너리(word_index)데이터를 생성\n",
    "# 사전의 첫 번째 인덱스에는 OOV를 설정\n",
    "tokenizer = preprocessing.text.Tokenizer(oov_token='OOV')\n",
    "tokenizer.fit_on_texts(d)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 5. 사전파일생성\n",
    "# 생성된 단어인덱스딕셔너리(word_index)객체를 파일로 저장\n",
    "f = open('./chatbot/train_tools/dict/chatbot_dict.bin', 'wb')\n",
    "try:\n",
    "    pickle.dump(word_index, f)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\train_tools\\dict\\create_dict_test.py\n",
    "# 단어사전테스트\n",
    "import pickle\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# 1. 단어사전로딩\n",
    "f = open('./chatbot/train_tools/dict/chatbot_dict.bin', 'rb')\n",
    "word_index = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "sent = \"내일 오전 10시에 탕수육 주문하고 싶어 ㅋㅋ\"\n",
    "\n",
    "# 2. 전처리객체생성\n",
    "p = Preprocess(userdic='../utils/user_dic.tsv')\n",
    "pos = p.pos(sent)\n",
    "\n",
    "# 3. 테스트문장을 입력값으로 키워드, index를 출력\n",
    "keywords = p.get_keywords(pos, without_tag=True)\n",
    "for word in keywords:\n",
    "    try:\n",
    "        print(word, word_index[word])\n",
    "    except KeyError:\n",
    "        # 해당단어가 사전에 없는 경우 OOV로 처리\n",
    "        print(word, word_index['OOV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\utils\\Preprocess.py\n",
    "# Preprocess.py(2) - 시퀀스생성로직 추가\n",
    "# 단어인덱스 시퀀스변환 메서드를 추가\n",
    "from konlpy.tag import Komoran\n",
    "import pickle\n",
    "import jpype\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, word2index_dic='', userdic=None):\n",
    "        # 단어 인덱스 사전 불러오기\n",
    "        if(word2index_dic != ''):\n",
    "            f = open(word2index_dic, \"rb\")\n",
    "            self.word_index = pickle.load(f)\n",
    "            f.close()\n",
    "        else:\n",
    "            self.word_index = None\n",
    "\n",
    "        # 형태소 분석기 초기화\n",
    "        self.komoran = Komoran(userdic=userdic)\n",
    "\n",
    "        # 제외할 품사\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언 제거, 기호 제거\n",
    "        # 어미 제거\n",
    "        # 접미사 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            'JX', 'JC', 'SF', 'SP', 'SS', 'SE', 'SO', 'EP', \n",
    "            'EF', 'EC', 'ETN', 'ETM', 'XSN', 'XSV', 'XSA']\n",
    "\n",
    "    # 형태소 분석기 POS 태거\n",
    "    def pos(self, sentence):\n",
    "        jpype.attachThreadToJVM()\n",
    "        return self.komoran.pos(sentence)\n",
    "\n",
    "    # 불용어 제거 후, 필요한 품사 정보만 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if f(p[1]) is False:\n",
    "                word_list.append(p if without_tag is False else p[0])\n",
    "        return word_list\n",
    "\n",
    "    # 키워드를 단어 인덱스 시퀀스로 변환\n",
    "    def get_wordidx_sequence(self, keywords):\n",
    "        if self.word_index is None:\n",
    "            return []\n",
    "\n",
    "        w2i = []\n",
    "        for word in keywords:\n",
    "            try:\n",
    "                w2i.append(self.word_index[word])\n",
    "            except KeyError:\n",
    "                # 해당 단어가 사전에 없는 경우, OOV 처리\n",
    "                w2i.append(self.word_index['OOV'])\n",
    "                \n",
    "        return w2i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf576b1",
   "metadata": {},
   "source": [
    "## 6.4 의도분류모델\n",
    "\n",
    "* 쳇봇엔진에 화자의 질의가 입력되었을 때 전처리과정을 거친후 `화자의 문장의 의도를 분류`해야 한다.\n",
    "* 클래스별로 분류하기위해서 `CNN모델을 사용`\n",
    "* 실습하는 말뭉치의 의도는 5가지분류\n",
    "\n",
    "<h6 align=\"center\">쳇봇 엔진의 의도 분류 클래스 종류</h6>\n",
    "\n",
    "|의도명|분류클래스|설명|\n",
    "|:------:|:---:|:----------------|\n",
    "|인사|0|텍스트가 인사말인 경우|\n",
    "|욕설|1|텍스트가 욕설인 경우|\n",
    "|주문|2|텍스트가 주문 관련 내용인 경우|\n",
    "|얘약|3|텍스트가 예약 관련 내용인 경우|\n",
    "|기타|4|어떤 의도에도 포함되지 않는 경우|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\config\\GlobalParams.py\n",
    "# 글로벌 파라미터정보\n",
    "# 단어 시퀀스 벡터 크기\n",
    "MAX_SEQ_LEN = 15\n",
    "\n",
    "def GlobalParams():\n",
    "    global MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05eece",
   "metadata": {},
   "source": [
    "### 6.4.1 의도분류모델학습\n",
    "\n",
    "* 학습데이터 : ./chatbot/models/intent/total_train_data.csv\n",
    "  - 음식주문과 예약을 위한 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b486ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .\\chatbot\\models\\intent\\train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile .\\chatbot\\models\\intent\\train_model.py\n",
    "# 쳇봇엔진-의도분류모델\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "# 1. 데이터로딩\n",
    "train_file = './chatbot/models/intent/total_train_data.csv'\n",
    "data = pd.read_csv(train_file, delimiter=\",\")\n",
    "queries = data['query'].tolist()\n",
    "intents = data['intent'].tolist()\n",
    "\n",
    "# 2. 단어시퀀스사전로딩\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "p = Preprocess(word2index_dic=\"./chatbot/train_tools/dict/chatbot_dict.bin\",\n",
    "               userdic=\"./chatbot/utils/user_dic.tsv\")\n",
    "\n",
    "# 3. 단어시퀀스 생성\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    pos = p.pos(sentence)\n",
    "    keywords = p.get_keywords(pos, without_tag=True)\n",
    "    seq = p.get_wordidx_sequence(keywords)\n",
    "    sequences.append(seq)\n",
    "    \n",
    "# 4. 단어인덱스시퀀스벡터생성\n",
    "# Preprocess에서 생성한 시퀀스벡터의 크기를 동일하게 처리하기 위해 \n",
    "# MAX_SEQ_LEN=15 즉 최대 벡터크기를 15로 설정하고 15이하인 벡터는 padding처리\n",
    "from chatbot.config.GlobalParams import MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "print(padded_seqs.shape, len(intents))\n",
    "print()\n",
    "\n",
    "# 5. 학습용, 검증용, 테스트용 데이터셋 생성 - 7:2:1\n",
    "# 패딩처리된 시퀀스(padded_seqs)리스트와 의도(intents)리스트를 데이터셋으로 생성후\n",
    "# 랜덤하게 학습용,검증용,테스트용 데이터셋을 7:2:1로 분할\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, intents))\n",
    "df = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "# 6. 하이퍼파라미터설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.word_index) + 1 \n",
    "\n",
    "# 7. CNN모델정의\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN, ))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "conv2 = Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "conv3 = Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "# 1) 3,4,5-gram 출력층을 한개로 합치기\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "# 2) 은닉층\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(5, name='logits')(dropout_hidden)\n",
    "predictions = Dense(5, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "# 8. CNN모델생성\n",
    "# 정의된 모델을 케라스모델에 추가, Model()함수는 입력층과 출력층만 사용\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 9. 모델학습\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
    "\n",
    "# 10. 모델평가\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print(f'Accuracy : {accuracy*100:.2f}')\n",
    "print(f'Loss     : {loss:.2f}')\n",
    "\n",
    "# 11. 모델저장\n",
    "model.save('./chatbot/models/intent/intent_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f99f06",
   "metadata": {},
   "source": [
    "### 6.4.2 의도분류모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1a4a363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .\\chatbot\\models\\intent\\IntentModel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile .\\chatbot\\models\\intent\\IntentModel.py\n",
    "# 쳇봇엔진 - 의도분류모델\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "# 의도분류모델모듈\n",
    "class IntentModel:\n",
    "    \n",
    "    def __init__(self, model_name, preprocess):\n",
    "        \n",
    "        # 의도클래스별 레이블\n",
    "        self.labels = {0: '인사', 1: '욕설', 2: '주문', 3: '예약', 4: '기타'}\n",
    "        \n",
    "        # 의도분류모델로딩\n",
    "        self.model = load_model(model_name)\n",
    "        \n",
    "        # 챗봇 Preprocess\n",
    "        self.p = preprocess\n",
    "    \n",
    "    # 의도클래스 예측 함수\n",
    "    def predict_class(self, query):\n",
    "        \n",
    "        # 형태소분석\n",
    "        pos = self.p.pos(query)\n",
    "        \n",
    "        # 문장내 키워드 출출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "    \n",
    "        # 벡터최대크기\n",
    "        from chatbot.config.GlobalParams import MAX_SEQ_LEN\n",
    "        \n",
    "        # 패딩처리\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "        \n",
    "        predict = self.model.predict(padded_seqs)\n",
    "        predict_class = tf.math.argmax(predict, axis=1)\n",
    "        \n",
    "        return predict_class.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd24432f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .\\chatbot\\models\\intent\\model_intent_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile .\\chatbot\\models\\intent\\model_intent_test.py\n",
    "# 쳇봇엔진 - 의도분류모델 테스트\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "from chatbot.models.intent.IntentModel import IntentModel\n",
    "\n",
    "p = Preprocess(word2index_dic=\"./chatbot/train_tools/dict/chatbot_dict.bin\",\n",
    "               userdic=\"./chatbot/utils/user_dic.tsv\")\n",
    "\n",
    "intent = IntentModel(model_name='./chatbot/models/intent/intent_model.keras', preprocess=p)\n",
    "\n",
    "query = '오늘 탕수육 주문 가능한가요?'\n",
    "predict = intent.predict_class(query)\n",
    "predict_label = intent.labels[predict]\n",
    "\n",
    "print(query)\n",
    "print(f'질의를 예측한 의도클래스 = {predict}')\n",
    "print(f'질의를 예측한 의도레이블 = {predict_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1ca55",
   "metadata": {},
   "source": [
    "## 6.5 개체명인식모델학습\n",
    "\n",
    "* 개체명인식모델을 `양방향LSTM모델을 사용`\n",
    "\n",
    "<h6 align=\"center\">개체명종류</h6>\n",
    "\n",
    "|개체명|설명|\n",
    "|:----:|:-----------------|\n",
    "|B_FOOD|음식|\n",
    "|B_DT, B_TI|날짜,시간(학습데이터의 영향으로 날짜와 시간을 혼용해서 사용)|\n",
    "|B_PS|사람|\n",
    "|B_OG|조직, 회사|\n",
    "|B_LC|지역|\n",
    "\n",
    "### 6.5.1 개체명인식모델 데이터셋\n",
    "\n",
    "* 학습용데이터셋 : ./chatbot/models/ner/ner_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e264388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 크기 : \n",
      " 61999\n",
      "0번 째 샘플 단어 시퀀스 : \n",
      " ['가락지빵', '주문', '하', '고', '싶', '어요']\n",
      "0번 째 샘플 bio 태그 : \n",
      " ['B_FOOD', 'O', 'O', 'O', 'O', 'O']\n",
      "샘플 단어 시퀀스 최대 길이 : 168\n",
      "샘플 단어 시퀀스 평균 길이 : 8.796238649010467\n"
     ]
    }
   ],
   "source": [
    "# %%writefile .\\chatbot\\models\\ner\\train_model.py\n",
    "# 쳇봇엔진 - NER모델작성\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# 1. 학습파일로딩\n",
    "def read_file(file_name):\n",
    "    sents = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, l in enumerate(lines):\n",
    "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
    "                this_sent = []\n",
    "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
    "                continue\n",
    "            elif l[0] == '\\n':\n",
    "                sents.append(this_sent)\n",
    "            else:\n",
    "                this_sent.append(tuple(l.split()))\n",
    "    return sents\n",
    "\n",
    "p = Preprocess(word2index_dic=\"./chatbot/train_tools/dict/chatbot_dict.bin\",\n",
    "               userdic=\"./chatbot/utils/user_dic.tsv\")\n",
    "\n",
    "# 2. 학습용말뭉치데이터 로딩\n",
    "corpus = read_file('./chatbot/models/ner/ner_train.txt')\n",
    "\n",
    "# 3. 말뭉치데이터에서 단어(2번쨰), BIO태그(4번째)만 로딩해서 학습용데이터셋을 생성\n",
    "sentences, tags = [], []\n",
    "for t in corpus:\n",
    "    tagged_sentence = []\n",
    "    sentence, bio_tag = [], []\n",
    "    for w in t:\n",
    "        tagged_sentence.append((w[1], w[3]))\n",
    "        sentence.append(w[1])\n",
    "        bio_tag.append(w[3])\n",
    "    \n",
    "    sentences.append(sentence)\n",
    "    tags.append(bio_tag)\n",
    "    \n",
    "print(\"샘플 크기 : \\n\", len(sentences))\n",
    "print(\"0번 째 샘플 단어 시퀀스 : \\n\", sentences[0])\n",
    "print(\"0번 째 샘플 bio 태그 : \\n\", tags[0])\n",
    "print(\"샘플 단어 시퀀스 최대 길이 :\", max(len(l) for l in sentences))\n",
    "print(\"샘플 단어 시퀀스 평균 길이 :\", (sum(map(len, sentences))/len(sentences)))    \n",
    "\n",
    "# 4. 토크나이저 정의 \n",
    "# 단어시퀀스는 Preprocess객체에서 생성하기 때문에 예제에서는 BIO태그용 토크나이저 객체만 생성\n",
    "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower=False 소문자로 변환하지 않는다.\n",
    "tag_tokenizer.fit_on_texts(tags)\n",
    "\n",
    "# 단어사전 및 태그 사전 크기\n",
    "vocab_size = len(p.word_index) + 1\n",
    "tag_size = len(tag_tokenizer.word_index) + 1\n",
    "print(\"BIO 태그 사전 크기 :\", tag_size)\n",
    "print(\"단어 사전 크기 :\", vocab_size)\n",
    "\n",
    "# 5. 학습용단어시퀀스생성\n",
    "# BIO태그는 토크나이저에서 생성된 사전데이터를 시퀀스번호형태로 인코딩한다.\n",
    "x_train = [p.get_wordidx_sequence(sent) for sent in sentences]\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "\n",
    "index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환 하기 위해 사용\n",
    "index_to_ner[0] = 'PAD'\n",
    "\n",
    "# 6. 시퀀스 패딩 처리\n",
    "# 개체명인식모델의 입출력크기를 동일하게 맞추기위해 시퀀스 패딩작업을 진행\n",
    "# 벡터크기를 단어 시퀀스의 평균길이보다 여유있게 40으로 정의\n",
    "max_len = 40\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=max_len)\n",
    "y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)\n",
    "\n",
    "# 7. 학습 데이터와 테스트 데이터를 8:2의 비율로 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,\n",
    "                                                    test_size=.2,\n",
    "                                                    random_state=1234)\n",
    "\n",
    "# 8. 출력 데이터를 one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
    "\n",
    "print(\"학습 샘플 시퀀스 형상 : \", x_train.shape)\n",
    "print(\"학습 샘플 레이블 형상 : \", y_train.shape)\n",
    "print(\"테스트 샘플 시퀀스 형상 : \", x_test.shape)\n",
    "print(\"테스트 샘플 레이블 형상 : \", y_test.shape)\n",
    "\n",
    "# 9. 모델정의(Bi-LSTM)\n",
    "# tag_size만큼 출력 뉴런에서 제일 확률 높은 출력값 1개를 선택하기 위해 활성화함수로 softmax를 사용\n",
    "# 손실함수는 categorical_crossentropy를 사용\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "\n",
    "print(\"평가 결과 : \", model.evaluate(x_test, y_test)so[1])\n",
    "model.save('./chatbot/models/ner/ner_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "769b002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "388/388 [==============================] - 162s 357ms/step - loss: 0.1143 - accuracy: 0.9681\n",
      "Epoch 2/10\n",
      "388/388 [==============================] - 137s 353ms/step - loss: 0.0397 - accuracy: 0.9871\n",
      "Epoch 3/10\n",
      "388/388 [==============================] - 140s 361ms/step - loss: 0.0272 - accuracy: 0.9911\n",
      "Epoch 4/10\n",
      "388/388 [==============================] - 140s 360ms/step - loss: 0.0211 - accuracy: 0.9930\n",
      "Epoch 5/10\n",
      "388/388 [==============================] - 140s 360ms/step - loss: 0.0179 - accuracy: 0.9940\n",
      "Epoch 6/10\n",
      "388/388 [==============================] - 135s 347ms/step - loss: 0.0160 - accuracy: 0.9946\n",
      "Epoch 7/10\n",
      "388/388 [==============================] - 129s 334ms/step - loss: 0.0153 - accuracy: 0.9946\n",
      "Epoch 8/10\n",
      "388/388 [==============================] - 125s 323ms/step - loss: 0.0139 - accuracy: 0.9952\n",
      "Epoch 9/10\n",
      "388/388 [==============================] - 126s 325ms/step - loss: 0.0132 - accuracy: 0.9954\n",
      "Epoch 10/10\n",
      "388/388 [==============================] - 127s 329ms/step - loss: 0.0128 - accuracy: 0.9955\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0547 - accuracy: 0.9856\n",
      "평가 결과 :  0.9855650067329407\n"
     ]
    }
   ],
   "source": [
    "# 9. 모델정의(Bi-LSTM)\n",
    "# tag_size만큼 출력 뉴런에서 제일 확률 높은 출력값 1개를 선택하기 위해 활성화함수로 softmax를 사용\n",
    "# 손실함수는 categorical_crossentropy를 사용\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "\n",
    "print(\"평가 결과 : \", model.evaluate(x_test, y_test)[1])\n",
    "model.save('./chatbot/models/ner/ner_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72048568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
