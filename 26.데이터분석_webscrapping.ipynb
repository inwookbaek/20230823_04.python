{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddaac9d7",
   "metadata": {},
   "source": [
    "# 웹스크래핑(Web Scrapping)\n",
    "\n",
    "## 1. 개념\n",
    "\n",
    "* 웹스크래핑(Web Scrapping) : 웹사이트상에서 특정부분에 위치한 정보를 컴퓨터로 자동추출하는 기능\n",
    "* 웹크롤링(Web Crawling) : 자동화봇이 정해진 규칙에 따라 복수개의 웹페이지를 브라우징하는 기능\n",
    "\n",
    "## 2. 필요한 라이브러리\n",
    "\n",
    "* `BeautifulSoup` : HTML과 XML문선를 파싱하기 위한 라이브러리\n",
    "* scrapy : Python을 작성된 오픈소스 웹크롤링 프레임워크\n",
    "\n",
    "## 3. 스크래핑(클롤링)하는 방법\n",
    "\n",
    "1. 원하는 페이지에 `요청(request)`을 보낸 후 그 결과를 html로 받는다.\n",
    "1. 받은 html을 `파싱(Parsing)`처리를 한다.\n",
    "1. 필요한 정보를 추출한다.\n",
    "1. 파이썬으로 스크래핑을 하기 위해서는 `http request/response 모듈`과 `html을 파싱하는 모듈`이 필요하다.\n",
    "\n",
    "##### 참고 사이트\n",
    "* https://www.crummy.com/software/BeautifulSoup\n",
    "* https://docs.python.org/3.0/library/urllib.request.html\n",
    "\n",
    "### 1. 웹스크래핑 -  html 소스읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de1dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. html 소스읽기\n",
    "from urllib.request import urlopen\n",
    "html = urlopen('http://www.google.com')\n",
    "print(type(html), html)\n",
    "print(dir(html))\n",
    "print()\n",
    "\n",
    "html.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184131b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 예외처리방법\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen('http://www.naverxx.com')\n",
    "except HTTPError as e:\n",
    "    print('HTTP 에러발생!!')\n",
    "except URLError as e:    \n",
    "    print('존재하지 않는 사이트 주소입니다!')\n",
    "else:\n",
    "    print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c53cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 이미지다운로드방법(1) - 간편한 방법\n",
    "import urllib.request\n",
    "\n",
    "# 다음사이트의 로고이미지를 다운로드하기\n",
    "# https://t1.daumcdn.net/daumtop_chanel/op/20200723055344399.png\n",
    "url = 'https://t1.daumcdn.net/daumtop_chanel/op/20200723055344399.png'\n",
    "save_file = './data/daum.png'\n",
    "\n",
    "urllib.request.urlretrieve(url, save_file)\n",
    "print('파일이 정상적으로 다운로드 되었습니다!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3330b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 이미지다운로드방법(2) - 바이너리파일로 처리\n",
    "image = urllib.request.urlopen(url).read()\n",
    "# print(type(image), image)\n",
    "\n",
    "# 바이너리로 저장\n",
    "with open('./data/daum_1.png', mode='wb') as f:\n",
    "    f.write(image)\n",
    "    print('파일이 정상적으로 다운로드 되었습니다!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 매개변수를 추가해서 인터넷의 자료를 요청하기\n",
    "# 기상청의 일기예보 사이트 : http://www.kma.go.kr\n",
    "# 기상청육상중기예보\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "\n",
    "API = 'https://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp'\n",
    "\n",
    "# url에 특수문자, 한글이 포함된 경우에는 URL인코딩이 필요\n",
    "# 지역번호\n",
    "#   전국 108, 서울/경기 109, 강원 105, 충북 131, 충남 133, 전북 146\n",
    "#   전남 156, 경북 143, 경남 159, 제주 184\n",
    "\n",
    "values = {'stnId':'108'} # 전국 날씨예보\n",
    "\n",
    "# url인코딩\n",
    "params = urllib.parse.urlencode(values)\n",
    "print(type(params), params)\n",
    "\n",
    "# 요청할 URL을 생성\n",
    "url = API + '?' + params\n",
    "print(url)\n",
    "\n",
    "# 응답된 자료 읽기\n",
    "data = urllib.request.urlopen(url).read() # 바이트타입으로 리턴\n",
    "#  print(type(data), data)\n",
    "\n",
    "# response데이터를 decoding\n",
    "text = data.decode('utf-8')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e17f362",
   "metadata": {},
   "source": [
    "## 2. 웹스크래핑 - BeautifulSoup\n",
    "\n",
    "* 다운로드 : https://www.crummy.com/software/BeautifulSoup\n",
    "* `pip install beautifulsoup4` or `conda install -y beautifulsoup4`\n",
    "* 주의) ~4를 생략하면 3.x버전이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47df246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BeautifulSoup : HTML을 파싱하는 Library\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# 1) url request\n",
    "# https://www.google.com의 메이페이지의 타이틀 텍스트 가져오기\n",
    "# url에 접속요청이 성공이 되면 HTTPResponse객체가 리턴된다.\n",
    "html = urlopen('https://www.google.com')\n",
    "print(type(html), html)\n",
    "\n",
    "# 2) html.read() : HTTPResponse에서 html 데이터를 읽기\n",
    "# BS의 기본분석기를 이용하여 html을 분석하기 위한 객체를 생성\n",
    "# html분석기 -> html, xml분석기 -> xml\n",
    "# html.read()\n",
    "soup = bs(html.read(), 'html.parser')\n",
    "# print(type(soup), soup)\n",
    "\n",
    "# 문서내의 첫 h1태그를 선택\n",
    "print(soup.h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ad820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. next_sibling\n",
    "\n",
    "# 분석하고자하는 html\n",
    "html ='''\n",
    "<html><body>\n",
    "    <h1>Hello Web Scraping</h1>\n",
    "    <p>웹 페이지 분석</p>\n",
    "    <p>웹 스크래핑</p>\n",
    "</body></html>\n",
    "'''\n",
    "soup = bs(html, 'html.parser')\n",
    "print(soup.h1, soup.p)\n",
    "\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "print(h1, p1, type(p1))\n",
    "\n",
    "# sibling 형제노드(동일레벨의 노드)\n",
    "# previous_sibiling : 동일레벨의 이전노드\n",
    "# next_sibling      : 동일레벨의 다음노드\n",
    "\n",
    "p2 = p1.previous_sibling.previous_sibling \n",
    "# 2번하는 이유는 h1, \\n\n",
    "# 즉,  p1.previous_sibling -> \\n(줄바꿈 문자) 선택\n",
    "#      p1.previous_sibling.previous_sibling  -> h1 tag선택\n",
    "print(p2)\n",
    "\n",
    "p2 = p1.next_sibling   # 줄바꿈문자선책\n",
    "p2 = p1.next_sibling.next_sibling # 2번째 p태그선택\n",
    "print(p2)\n",
    "print()\n",
    "\n",
    "# Tag의 내부 text 추출하기\n",
    "print(f'h1 = {h1.string}')\n",
    "print(f'p1 = {p1.string}')\n",
    "print(f'p2 = {p2.string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55459e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. find()\n",
    "html ='''\n",
    "<html><body>\n",
    "    <h1 id=\"title\">Hello Web Scraping</h1>\n",
    "    <p id=\"body\">웹 페이지 분석</p>\n",
    "    <p>웹 스크래핑</p>\n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "soup = bs(html, 'html.parser')\n",
    "# print(dir(soup))\n",
    "# soup.find?\n",
    "# 실습. id title인 태그, id가 body인 태그를 선택하고\n",
    "# 내부문자열을 출력해보기\n",
    "title = soup.find(id='title')\n",
    "body = soup.find(id='body')\n",
    "print(title, body)\n",
    "print()\n",
    "\n",
    "print(f'title = {title.string}')\n",
    "print(f'body = {body.string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. find_all() : 특정속성을 가진 tag 전부를 추출하기\n",
    "html ='''\n",
    "<html><body>\n",
    "    <li><a href=\"http://daum.net\">daum</a></li>\n",
    "    <li><a href=\"http://google.com\">google</a></li>\n",
    "    <li><a href=\"http://yahoo.com\">yahoo</a></li>\n",
    "    <li><a href=\"http://nate.com\">nate</a></li>\n",
    "    <li><a href=\"http://naver.com\">naver</a></li>\n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "# 실습2. a태그 전체를 추출하기\n",
    "# 전체를 가져올경우 데이터의 타입확인(bs4.element.ResultSet) \n",
    "# list자료형과 동일, list는 for문을 이용해서 출력\n",
    "\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "links = soup.find_all('a')\n",
    "print(type(links), links)\n",
    "print()\n",
    "\n",
    "for link in links:\n",
    "    # print(type(link), link)\n",
    "    text = link.string\n",
    "    # 태그내부의 속성 가져오기 : attrs\n",
    "    href = link.attrs['href']\n",
    "    print(f'{text} = {href}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97268f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. CSS처리 : select_one(), select()\n",
    "# 실습. h1의 도서목록을 추출하기 -> select_one() \n",
    "#       ... css선택자 : div#main > h1 \n",
    "#       li목록 -> select()\n",
    "#       ... css선택자 : div#main > ul > li\n",
    "html ='''\n",
    "<html><body>\n",
    "    <div id=\"main\">\n",
    "        <h1>도서목록</h1>\n",
    "        <ul>\n",
    "            <li>자바프로그램 입문</li>\n",
    "            <li>파이썬으로 하는 데이터분석</li>\n",
    "            <li>HTML5/CSS3</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</body></html>\n",
    "'''\n",
    "soup = bs(html, 'html.parser')\n",
    "# soup.select?\n",
    "# soup.select_one?\n",
    "\n",
    "h1 = soup.select_one('div#main > h1').string\n",
    "print(soup.h1.string, '=', h1)\n",
    "\n",
    "# li목록 : select()\n",
    "li_list = soup.select('div#main > ul > li')\n",
    "print(type(li_list), li_list)\n",
    "print()\n",
    "\n",
    "for li in li_list:\n",
    "    print(f'li = {li.string}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4adf58",
   "metadata": {},
   "source": [
    "#### 실습1. 기상청의 일기예보\n",
    "![CDATA[○ (하늘상태, 강수) 이번 예보기간에는 전국이 가끔 구름많겠으나, 10일(금) 오전 제주도에 비가 오겠습니다. <br />○ (기온) 9일(목)~10일(금) 아침 기온은 4~15도, 낮 기온은 10~21도로 평년(최저기온 0~10도, 최고기온 11~18도)보다 조금 높겠으며,<br />          11일(토)~16일(목) 아침 기온은 -4~9도, 낮 기온은 7~17도로 평년과 비슷하거나 조금 낮겠습니다.<br />○ (주말전망) 11일(토)은 오전에 구름많다가 오후에 대체로 맑겠으나 강원영동과 제주도는 대체로 흐리겠습니다. 12일(일)은 중부지방은 대체로 맑겠고, 남부지방은 구름많겠습니다.<br />              아침 기온은 -2~9도, 낮 기온은 7~17도가 되겠습니다.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db689cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req \n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "stnId = input('지역번호를 입력하세요 => ')\n",
    "url = f'https://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId={stnId}'\n",
    "print(url)\n",
    "\n",
    "res = req.urlopen(url)\n",
    "soup = bs(res, 'html.parser')\n",
    "\n",
    "title = soup.find('title').string\n",
    "print(title)\n",
    "print()\n",
    "\n",
    "wf = soup.find('wf').string\n",
    "wf = wf.replace('<br />', '')\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c7f0f",
   "metadata": {},
   "source": [
    "#### 실습2. 네이버금융에서 환율정보 추출하기\n",
    "\n",
    "* http://finance.naver.com/marketindex\n",
    "* 출력 usd/krw = 1302.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://finance.naver.com/marketindex'\n",
    "res = req.urlopen(url)\n",
    "soup = bs(res, 'html.parser')\n",
    "# <div class=\"head_info point_dn\">\n",
    "# <span class=\"value\">1,302.20</span>\n",
    "\n",
    "ex_rate = soup.select_one('div.head_info > span.value')\n",
    "# print(ex_rate)\n",
    "print(f'usd/krw = {ex_rate.string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255032f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_rates = soup.select('div.head_info > span.value')\n",
    "print(type(ex_rate), ex_rate)\n",
    "print()\n",
    "for rate in ex_rates:\n",
    "    print(rate.string) # 통화별 환율구분해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1aec4a",
   "metadata": {},
   "source": [
    "#### 실습3. 윤동주시인의 작품목록 추출하기\n",
    "\n",
    "* https://ko.wikisource.org/wiki/저자:윤동주\n",
    "  -- 한글주소상태로 복사하기 - 크롬웹스토어에서 `copy unicode urls` 설치\n",
    "* select() \n",
    "* 웹브라우저에서 F12개발자도구, elements에서 원하는 부분을 우클릭\n",
    "  - Copy > Copy selector : CSS 셀렉터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC'\n",
    "res = req.urlopen(url)\n",
    "soup = bs(res, 'html.parser')\n",
    "\n",
    "a_list = soup.select('.mw-parser-output > ul > li > a')\n",
    "# print(type(a_list), len(a_list), a_list)\n",
    "\n",
    "for a in a_list:\n",
    "    name = a.string\n",
    "    print(f'... {name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8118520",
   "metadata": {},
   "source": [
    "### 3. BeautifulSoup 관련함수\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "<p class=\"story\">\n",
    "    Once upon a time there were three little sisters; and their names were\n",
    "    <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "    <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "    <a href=\"http://example.com/tillie\" class=\"sister brother\" id=\"link3\">Tillie</a>;\n",
    "    and they lived at the bottom of a well.\n",
    "</p>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e230a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req \n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15665e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head><title>The Dormouse's story</title></head>\n",
    "    <body>\n",
    "    <p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "    <p class=\"story\">\n",
    "        Once upon a time there were three little sisters; and their names were\n",
    "        <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "        <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "        <a href=\"http://example.com/tillie\" class=\"sister brother\" id=\"link3\">Tillie</a>;\n",
    "        and they lived at the bottom of a well.\n",
    "    </p>\n",
    "    </body>\n",
    "    </html>\n",
    "'''\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# 1. element\n",
    "print(soup.title, soup.find('title'))\n",
    "\n",
    "# 2. tag 이름\n",
    "print(soup.title.name, soup.find('title').name)\n",
    "\n",
    "# 3. text\n",
    "print(soup.title.string, soup.find('title').string, soup.title.get_text())\n",
    "\n",
    "# 4. single element\n",
    "print(soup.a.get_text(), soup.find('a').string, soup.find('a').get_text())\n",
    "\n",
    "# 5. multi element\n",
    "print(soup.find_all('a'))\n",
    "print(soup.find_all('a')[0].get_text(),soup.find_all('a')[1].get_text(),soup.find_all('a')[2].get_text())\n",
    "\n",
    "# 6. attribute\n",
    "print(soup.a['class'], soup.a.get('class'))\n",
    "print(soup.a['href'], soup.a.get('href'))\n",
    "\n",
    "# 7. find by id\n",
    "print(soup.find(id='link1'))\n",
    "print(soup.find('a', {'id':'link1'}))\n",
    "print()\n",
    "\n",
    "# 8. find by class\n",
    "print(soup.find(class_='sister'))\n",
    "print(soup.find('a', {'class':'sister'}))\n",
    "print(soup.find_all('a', {'class':'sister'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd211d",
   "metadata": {},
   "source": [
    "#### 4. 고급 - HTML 분석하기\n",
    "\n",
    "* https://github.com/REMitchell/python-scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede962b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 전쟁과 평화\n",
    "html = req.urlopen('http://pythonscraping.com/pages/warandpeace.html')\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebebd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 1) span태그에서 class가 green인 태그 : 등장인물의 이름을 출력\n",
    "# findAll(), find_all()\n",
    "names = soup.findAll('span', {'class':'green'})\n",
    "for name in names:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f5d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 2) 웹페이지에서 모든 heading태그(h1~h6)의 text를 출력\n",
    "#         - list안에 for문을 사용\n",
    "titles = soup.find_all(['h1','h2','h3','h4','h5','h6',])\n",
    "print([title for title in titles])\n",
    "print([title.string for title in titles])\n",
    "print([title.get_text() for title in titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58333ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 3) span태그에서 class가 green or red인 태그의 text출력\n",
    "#         - find_all('', {'class': {집합}), list안에 for문\n",
    "all_text = soup.find_all('span', {'class': ['green', 'red']})\n",
    "print([text.string for text in all_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59654267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 4) wordcount = 'the prince'단어의 갯수를 출력\n",
    "wordcount = soup.find_all(text='the prince')\n",
    "print(type(wordcount), len(wordcount), wordcount)\n",
    "print(f'the prince의 출현갯수 = {len(wordcount)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cddbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자식노드 추출하기\n",
    "html = req.urlopen('http://pythonscraping.com/pages/page3.html')\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 5) table태그중에서 id가 giftList인 태그의 자식노드를 추출\n",
    "# hint) 자식노드 : soup.find().children, soup.find().tr.previous_sibling, next...\n",
    "a = soup.find('table', {'id':'giftList'}).children\n",
    "print(type(a))\n",
    "\n",
    "for child in soup.find('table', {'id':'giftList'}).children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cd491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 6) next_siblings\n",
    "# table태그에 id giftList인 태그의 자식노드 추출\n",
    "# hint) soup.find().tr.next_siblings\n",
    "for sibling in soup.find('table', {'id':'giftList'}).tr.next_siblings:\n",
    "    print(sibling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 7) previous_sibling\n",
    "# img1.jpg의 부모노드의 이전형제노드의 text추출하기\n",
    "# sound.find().parent.previous_sibling을 이용\n",
    "print(soup.find('img', {'src':'../img/gifts/img1.jpg'}))\n",
    "print(soup.find('img', {'src':'../img/gifts/img1.jpg'}).parent.previous_sibling)\n",
    "print(soup.find('img', {'src':'../img/gifts/img1.jpg'}).parent.previous_sibling.string)\n",
    "print(soup.find('img', {'src':'../img/gifts/img1.jpg'}).parent.previous_sibling.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습8) 정규식을 사용 - img태그 전체를 추출하기\n",
    "# re.complie(../img/gifts/img*.jpg)\n",
    "import re\n",
    "imgs = soup.find_all('img', {'src': re.compile('\\.\\.\\/img\\/gifts\\/img.*\\.jpg')})\n",
    "\n",
    "for img in imgs:\n",
    "    print(img['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 9) lambda식이용\n",
    "\n",
    "# a. tag속성이 2개인 것만 추출\n",
    "print(soup.find_all(lambda tag: len(tag.attrs)>=2))\n",
    "print()\n",
    "\n",
    "# b. text = 'Or maybe he\\'s only resting?'만 추출\n",
    "print(soup.find_all(lambda tag: tag.get_text() == 'Or maybe he\\'s only resting?'))\n",
    "print(soup.find_all('', text='Or maybe he\\'s only resting?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6fd76c",
   "metadata": {},
   "source": [
    "### 5. json분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json탐색 : {key:value}\n",
    "import json\n",
    "json_str = '''{\n",
    "    \"amount\":[{\"num\":0},{\"num\":1},{\"num\":2}], \n",
    "    \"fruits\":[{\"fruit\":\"apple\"},{\"fruit\":\"banana\"},{\"fruit\":\"pear\"}]\n",
    "}'''\n",
    "print(type(json_str), json_str)\n",
    "\n",
    "# json을 dict 변환\n",
    "dict_data = json.loads(json_str) \n",
    "print(type(dict_data), dict_data)\n",
    "print()\n",
    "\n",
    "print(type(dict_data.get('fruits')), dict_data.get('fruits'))\n",
    "print(dict_data.get('fruits')[0])\n",
    "print(dict_data.get('fruits')[1])\n",
    "print(dict_data.get('fruits')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbfe56a",
   "metadata": {},
   "source": [
    "### 6. pdf 분석\n",
    "\n",
    "* `pip install pdfminer3k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53584dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfminer3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2bfcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show pdfminer3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, process_pdf\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "from io import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8eda12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req \n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPDF(pdfFile):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    \n",
    "    process_pdf(rsrcmgr, device, pdfFile)\n",
    "    device.close()\n",
    "    \n",
    "    content = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81786526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전쟁과 평화\n",
    "pdfFile = req.urlopen('https://pythonscraping.com/pages/warandpeace/chapter1.pdf')\n",
    "print(type(pdfFile), pdfFile)\n",
    "\n",
    "content = readPDF(pdfFile)\n",
    "print(type(content), content)\n",
    "pdfFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1986a",
   "metadata": {},
   "source": [
    "#### 7. Data저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹페이지의 내용을 분석하여 csv파일로 저장\n",
    "# table태그의 내부 텍스트를 저장\n",
    "import csv\n",
    "\n",
    "html = req.urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# class가 'wikitable'인 태그중에서 첫 번째 tag를 선택\n",
    "table = soup.findAll('table', {'class': 'wikitable'})[0]\n",
    "# print(type(table), table)\n",
    "rows = table.findAll('tr')\n",
    "# print(rows)\n",
    "\n",
    "csv_file = open('./data/web/editors.csv', 'wt', newline='', encoding='utf-8')\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "try:\n",
    "    for row in rows:\n",
    "        csv_row = []\n",
    "        # td, th태그이 내용을 list에 추가\n",
    "        for cell in row.findAll(['td', 'th']):\n",
    "            csv_row.append(cell.get_text())\n",
    "        writer.writerow(csv_row)\n",
    "except Exception as e:\n",
    "    print('파일입출력에 에러가 발생했습니다!')\n",
    "finally:\n",
    "    print('파일이 성공적으로 저장이 되었습니다!')\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a91c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve, urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com')\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "img = soup.find('a', {'id':'logo'}).find('img')['src']\n",
    "urlretrieve(img, 'logo.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d1997",
   "metadata": {},
   "source": [
    "연습문제 1.\n",
    "다음 사이트에서 링크가 되어 있는 모든 제목을 가져와서 출력하기\n",
    "* http://media.daum.net/digital/\n",
    "\n",
    "연습문제 2.\n",
    "네이버사이트 이미지 검색후 '../data/web'에 한번에 다운로드 및 저장하기(10건정도)\n",
    "* 검색어를 전달받아서 결과를 저장\n",
    "* https://search.naver.com/search.naver?sm=tab_hty.top&where=image&query=코로나"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dca419",
   "metadata": {},
   "source": [
    "#### 8. 웹사이트정보를 mysql로 저장\n",
    "\n",
    "##### 1. mysql database와 table 생성\n",
    "```sql\n",
    "create database pyweb default character set utf8;\n",
    "\n",
    "create table pages(\n",
    "    id int not null auto_increment primary key,\n",
    "    title varchar(200),\n",
    "    content text,\n",
    "    created timestamp default current_timestamp\n",
    ");  \n",
    "```\n",
    "##### 2. mysql 연결하기\n",
    ">* 설치 : \n",
    ">>* pip install pymysql 또는\n",
    ">>* conda install pymysql\n",
    "* 연결 : `pymysql.connect(host='localhost', port=3306, user='root', passwd='12345', db='pyweb', charset='utf8')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib.request as req\n",
    "import re\n",
    "import datetime\n",
    "import random\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(host='localhost', port=3306\n",
    "                     , user='root', passwd='12345'\n",
    "                     , db='pyweb', charset='utf8')\n",
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac2bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n",
    "cursor.execute('use pyweb')\n",
    "cursor.execute('delete from pages')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# 테이블에 데이터를 저장하는 함수\n",
    "def store(title, content):\n",
    "    # 따옴표처리\n",
    "    title = title.replace(\"'\", \"''\") # 작은 따옴표1개를 2개로 치환\n",
    "    title = title.replace('\"', \"\\'\") # 큰 따옴표는 \\'로 치환\n",
    "    content = content.replace(\"'\", \"''\")\n",
    "    content = content.replace('\"', \"\\'\")\n",
    "    content = content.replace('\\n', '') # 줄바꿈문자는 제거\n",
    "    \n",
    "    sql = 'insert into pages(title, content) values(\"%s\", \"%s\")' % (title,content)\n",
    "    cursor.execute(sql)\n",
    "    conn.commit()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d51ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹사이트를 링크\n",
    "def getLinks(url):\n",
    "    html = req.urlopen('https://en.wikipedia.org' + url)\n",
    "    soup = bs(html, 'html.parser')\n",
    "    title = soup.find('h1').get_text()\n",
    "    content = soup.find('div', {'id':'mw-content-text'}).find('p').get_text()\n",
    "    store(title, content)\n",
    "    \n",
    "    # ^: 시작, $: 끝, *: 반복\n",
    "    # ^(/wiki/) : /wiki/ 로 시작되는 a 태그를 찾기\n",
    "    # ((?!:).) : .로 끝나는 문자에서 ?,!,:를 검색\n",
    "    # 전체의미는 /wiki/ + 문자열 + .의 패턴에서 문자열중에 ?,!,:를 검색\n",
    "    return soup.find('div', {'id':'bodyContent'}) \\\n",
    "               .findAll('a', href=re.compile(\"^(/wiki/)((?!:).)*$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(host='localhost', port=3306\n",
    "                     , user='root', passwd='12345'\n",
    "                     , db='pyweb', charset='utf8')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('use pyweb')\n",
    "cursor.execute('delete from pages')\n",
    "conn.commit()\n",
    "\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "print(links, len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d375105",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    count = 0\n",
    "    while len(links) > 0:\n",
    "        idx = random.randint(0, len(links) - 1)\n",
    "        newArticle = links[idx].attrs['href']\n",
    "        print(newArticle, idx)\n",
    "        link = getLinks(newArticle)\n",
    "        count += 1\n",
    "        if count >=5:\n",
    "            break\n",
    "except Exception as e:\n",
    "    print('저장실패!!!')\n",
    "else:\n",
    "    print('저장성공!!')\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2709411",
   "metadata": {},
   "source": [
    "#### 9. 웹사이트 캡춰\n",
    "\n",
    "##### 1. phantomjs\n",
    "\n",
    "1. http://phantomjs.org/download.html\n",
    "1. phantomjs-2.1.1-windows.zip\n",
    "1. 04.python\\phantomjs에 압축해제(\n",
    "\n",
    "##### 2. selenium\n",
    "\n",
    "1. `pip install selenium` or `conda install -y selenium`\n",
    "1. 웹브라우저를 컨트롤하여 자동화시켜주는 도구\n",
    "   - `webdriver`라는 API를 통해 운영체제에 설치된 chrome등의 웹브라우저를 제어\n",
    "1. 제공하는 기능\n",
    "   - url에 접근하는 api : `get(url)`\n",
    "   - 페이지의 단일 엘리먼트에 접근하는 api\n",
    "     - `find_element_by_id(html_id)`\n",
    "     - `find_element_by_name(html_name)`\n",
    "     - `find_element_by_xpath(html/body/div/img)`\n",
    "   - 페이지의 복수 엘리먼트에 접근하는 api\n",
    "     - `find_element_by_css_selector('#id > div.selector')`\n",
    "     - `find_element_by_css_name('클래스명')`\n",
    "     - `find_element_by_tag_name('h1')`\n",
    "1. selenium을 사용하려면 해당 웹브라우저의 webdriver를 다운해야 한다.\n",
    "   - 웹브라우저에 맞는 드라이버를 설치 해야 한다.\n",
    "     - 크롬의 경우 웹브라우저의 버전을 확인 -> 119.????,??\n",
    "     - 119버전에 맞는 webdriver(chromedeiver.exe)를 다운\n",
    "       -https://googlechromelabs.github.io/chrome-for-testing/#stable\n",
    "   - Chrome : https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "   - Edge : https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/\n",
    "   - Firefox : https://github.com/mozilla/geckodriver/releases\n",
    "   - 크롬만 다운\n",
    "     - 04.python에 압축해제\n",
    "     \n",
    "1. selenium 4.x 버전\n",
    "   * https://www.selenium.dev/documentation/webdriver/troubleshooting/upgrade_to_selenium_4/\n",
    "   * 참고사이트 : * https://blog.naver.com/hi_eng_/222928621005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93202082",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium         # 최신버전 설치\n",
    "# !pip install selenium==3.8.0  # 특정 버전을 설치\n",
    "# !pip install --upgrade selenium  # 현재버전에서 최신버전\n",
    "!pip show selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d54c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyperclip\n",
    "!pip show pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = '네이id'\n",
    "PASS = '네이버PW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c89ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a95f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()  # 크롬웹브라우저를 생성\n",
    "driver.get('http://www.naver.com') # 로그인하기 위한 사이트주소\n",
    "time.sleep(3)   # 3초지연(대기시간)\n",
    "\n",
    "# 로그인버튼 클릭\n",
    "login_btn = driver.find_element(By.CLASS_NAME, 'MyView-module__link_login___HpHMW')\n",
    "login_btn.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# id, pw를 clear \n",
    "tag_id = driver.find_element(By.ID, 'id')\n",
    "tag_pw = driver.find_element(By.ID, 'pw')\n",
    "tag_id.clear()\n",
    "tag_pw.clear()\n",
    "time.sleep(1)\n",
    "\n",
    "# id, pw를 자동으로 입력\n",
    "tag_id.click() # focus이동\n",
    "pyperclip.copy(USER)                  # 클립보드로 카피\n",
    "tag_id.send_keys(Keys.CONTROL, 'v')   # ctrl+v 클릭\n",
    "time.sleep(1)\n",
    "\n",
    "tag_pw.click() # focus이동\n",
    "pyperclip.copy(PASS)                  # 클립보드로 카피\n",
    "tag_pw.send_keys(Keys.CONTROL, 'v')   # ctrl+v 클릭\n",
    "time.sleep(1)\n",
    "\n",
    "# 로그인버튼클릭\n",
    "login_btn = driver.find_element(By.ID, 'log.login')\n",
    "login_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d2257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습. 네이버쇼핑 구매내역조회하기\n",
    "driver.get(\"https://order.pay.naver.com/home?tabMenu=SHOPPING\")\n",
    "prod_lists = driver.find_element(By.CLASS_NAME, 'PaymentList_link-detail__I74mH')\n",
    "print(type(prod_lists), prod_lists)\n",
    "\n",
    "# for product in prod_lists:\n",
    "#     pname = product.find_element(By.CSS_SELECTOR, 'p').text\n",
    "#     print(pname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30701162",
   "metadata": {},
   "source": [
    "##### 실습1. 교보문고에서 도서목록 수집하기\n",
    "\n",
    "* url : https://search.kyobobook.co.kr/web/search?vPstrKeyWord={url.parse.quote(book_name)}\n",
    "  - \n",
    "* 책 제목이 파이썬인 도서를 출력하세요!\n",
    ">22권이 검색되었습니다\n",
    ">- 혼자 공부하는 파이썬\n",
    ">- Do it! 점프 투 파이썬\n",
    ">- 두근두근 파이썬\n",
    "> Python Essential Reference, 2/e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ae2e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 권이 검색되었습니다!\n",
      "------------------------------------------------------------\n",
      "파이썬 텍스트 마이닝 바이블 1\n",
      "파이썬 텍스트 마이닝 바이블 2\n",
      "코딩 테스트 합격자 되기: 파이썬 편\n",
      "작심 3일 파이썬 Python\n",
      "2023 빅데이터분석기사 실기 한권완성 파이썬(Python)\n",
      "초보자를 위한 파이썬(Python) 200제\n",
      "Getting Start Python(파이썬)\n",
      "파이썬 딥러닝 파이토치(Python Deep Learning PyTorch)\n",
      "파이썬(Python) 3학년 머신러닝의 구조\n",
      "파이썬 클린 코드\n",
      "고성능 파이썬\n",
      "러닝 파이썬(하)\n",
      "러닝 파이썬(상)\n",
      "전문가를 위한 파이썬 프로그래밍\n",
      "처음 시작하는 파이썬\n",
      "Black Hat Python\n",
      "실전 파이썬 핸즈온 프로젝트\n",
      "파이썬 매일 코딩\n",
      "파이썬과 수치 해석\n",
      "파이썬으로 공부하는 블록체인\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "\n",
    "# 교보사이트에서 python검색\n",
    "html = urlopen('https://search.kyobobook.co.kr/search?keyword=python&gbCode=TOT&target=total')\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# select()\n",
    "# #shopData_list > ul > li:nth-child(1) > div > div.prod_info_box > span > a > span:nth-child(2)\n",
    "titles = soup.select('a.prod_info > span')\n",
    "# print(type(titles), titles)\n",
    "\n",
    "print(f'{len(titles)} 권이 검색되었습니다!')\n",
    "print('-' * 60)\n",
    "\n",
    "# print(titles)\n",
    "\n",
    "for title in titles:\n",
    "    if(title.text[0] !='[' and title.text[0] !='예'):\n",
    "        print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "194a058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색할 도서명을 입력하세요 => 머신러닝\n",
      "55 권이 검색되었습니다!\n",
      "------------------------------------------------------------\n",
      "머신 러닝(Machine Learning)\n",
      "이보다 더 쉬울 수 없는 자바 머신러닝 with Weka\n",
      "핸즈온 머신러닝\n",
      "실전! 컴퓨터비전을 위한 머신러닝\n",
      "머신 러닝 교과서 with 파이썬, 사이킷런, 텐서플로\n",
      "파이썬 라이브러리를 활용한 머신러닝\n",
      "금융 전략을 위한 머신러닝\n",
      "그래프 머신러닝\n",
      "살아 움직이는 머신러닝 파이프라인 설계\n",
      "R을 활용한 머신러닝\n",
      "그림으로 배우는 StatQuest 머신러닝 강의\n",
      "머신러닝 디자인 패턴\n",
      "머신러닝 파워드 애플리케이션\n",
      "비즈니스 머신러닝\n",
      "핸즈온 머신러닝\n",
      "사이버 보안을 위한 머신러닝 쿡북\n",
      "자산운용을 위한 금융 머신러닝\n",
      "AWS 클라우드 머신러닝\n",
      "통계학으로 배우는 머신러닝\n",
      "금융 전문가를 위한 머신러닝 알고리즘\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "def book_search(book_name):\n",
    "    html = urlopen(f'https://search.kyobobook.co.kr/web/search?vPstrKeyWord={urllib.parse.quote(book_name)}')\n",
    "    soup = bs(html, 'html.parser')   \n",
    "    \n",
    "    books = soup.select('a.prod_info > span')\n",
    "    print(f'{len(books)} 권이 검색되었습니다!')\n",
    "    print('-' * 60)\n",
    "    \n",
    "    for book in books:\n",
    "        if(book.text[0] !='[' and book.text[0] !='예'):\n",
    "            print(book.text)\n",
    "            \n",
    "book_name = input('검색할 도서명을 입력하세요 => ')\n",
    "book_search(book_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c8e8b",
   "metadata": {},
   "source": [
    "##### 연습문제. 본인이 scraping하는 사이트를 선정해서 검색추출결과를 출력하기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
